# AI Translation Evaluation Pipeline

This repository contains a clean, well-documented Jupyter notebook for evaluating machine translation outputs produced by multiple **AI models**. The pipeline parses `.txt` files into **Source** and **Output**, prepares per-model hypothesis files, computes **reference-free Quality Estimation (QE)** scores with COMET, and performs **paired t-tests** to compare hypotheses.

---

## Contents

- `ai_translation__evaluation_clean.ipynb` — the cleaned, commented evaluation notebook.
- `AI/` — base directory expected to contain one subfolder per AI model (see **Folder Structure**).

> Replace `AI/` with your actual base directory if different.

---

## Folder Structure

Place your data under a single base directory (default: `AI/`). Each **AI model** has its own subfolder with `.txt` files.

```
AI/
├── ModelA/
│   ├── 01.txt
│   ├── 02.txt
│   └── ...
├── ModelB/
│   ├── 01.txt
│   ├── 02.txt
│   └── ...
└── ModelC/
    ├── 01.txt
    ├── 02.txt
    └── ...
```

### Expected `.txt` format

Each file must contain the **source text** and the model **output** separated by a delimiter. The notebook looks for the first match among:

- `Output:`
- `output:`

Example:

```
[Optional headers or context]

This is the source sentence or paragraph.

Output:
This is the translation generated by the AI model.
```

> The split is case-sensitive for the capitalized and lowercase variants above. If neither is present, the entire file is treated as **source** and the hypothesis will be empty.

---

## What the Notebook Does

1. **Environment Setup**
   - Optional Google Drive mount (when run in Google Colab).
   - Idempotent installation of dependencies (COMET QE).

2. **Discovery**
   - Scans the base directory to find subfolders that contain `.txt` files (each treated as one **AI model**).

3. **Parsing**
   - For each `.txt` file, splits content into **Source** and **Output** using the first occurrence of `Output:` (or `output:`).

4. **Hypothesis Preparation**
   - Concatenates per-model sources into `src.txt`.
   - Concatenates per-model outputs into `hypX.txt` where `X` is the hypothesis index assigned to that model.

5. **Quality Estimation (QE)**
   - Uses a reference-free COMET QE model (`wmt21-comet-qe-mqm`) to score each `(source, hypothesis)` pair at **file level**.

6. **Aggregation**
   - Builds a tidy table with per-file QE scores and a wide-form table with one column per hypothesis (`hyp1`, `hyp2`, ...).

7. **Statistical Testing**
   - Runs **paired t-tests** between all hypothesis pairs on matched files to check for mean score differences.

8. **Artifacts**
   - Saves per-file and wide-form scores as CSVs.
   - Saves paired t-test results as an Excel file.

---

## Outputs

The notebook writes results to the base directory (default: `AI/`):

- `qe_scores_per_file.csv` — per-file QE scores with columns: `model`, `file`, `qe_score`, `hypothesis`.
- `qe_scores_wide.csv` — wide-form scores with one column per hypothesis and rows indexed by file.
- `paired_ttest_results.xlsx` — statistics for each hypothesis pair:
  - `Hypothesis 1`, `Hypothesis 2`, `t-statistic`, `p-value`, `n`.

Additionally, for each model folder:

- `src.txt` — one source line per original file in that model’s folder.
- `hypX.txt` — one output line per original file in that model’s folder (X is the hypothesis index).

---

## Dependencies

- Python ≥ 3.9
- `pandas`
- `scipy`
- `unbabel-comet` (COMET QE)

The notebook installs `unbabel-comet` automatically if missing.

### Optional (for Colab)
- `google.colab`

---

## Quick Start

### Option A — Google Colab

1. Upload `ai_translation__evaluation_clean.ipynb` to Colab.
2. Ensure your data exists in Drive, e.g., `MyDrive/AI/ModelA/*.txt`, etc.
3. In the notebook:
   - The Drive mount cell will run automatically.
   - Set `BASE_DIR` to your Drive path (default: `/content/drive/MyDrive/AI`).
4. Run all cells.

### Option B — Local

1. Place `AI/` alongside the notebook (or adjust `BASE_DIR` accordingly).
2. Create a virtual environment and install dependencies:
   ```bash
   python -m venv .venv
   source .venv/bin/activate      # Windows: .venv\\Scripts\\activate
   pip install -U pip
   pip install pandas scipy unbabel-comet
   ```
3. Open the notebook (VS Code, Jupyter, etc.), set `BASE_DIR = "./AI"` (or your path), and run all cells.

---

## Configuration

All key settings live near the top of the notebook:

- **Base Directory**
  - `BASE_DIR` — points to the folder that contains subfolders per model.

- **Parsing Delimiters**
  - `PARSE_KEYS = ["Output:", "output:"]`
  - Add more variants if your files use different markers.

- **Hypothesis Labels**
  - `HYP_LABELS = {}`
  - Optional mapping from model folder names to labels (e.g., `{"ModelA": "hyp1"}`).
  - If empty, labels are assigned as `hyp1`, `hyp2`, … by alphabetical order of model folder names.

---

## Evaluation Methodology

- **Quality Estimation (QE):** The notebook uses a reference-free COMET QE model (`wmt21-comet-qe-mqm`) to score `(source, hypothesis)` pairs at the sentence/file level. COMET QE correlates with human judgments without requiring a reference translation.

- **Statistical Testing:** The notebook performs **two-tailed paired t-tests** between all hypothesis pairs on matched files to test whether mean QE scores differ.
  - Assumes the differences are approximately normally distributed.
  - Reports `t-statistic`, `p-value`, and the sample size `n` after dropping pairs with missing scores.

> You can augment with non-parametric tests (e.g., Wilcoxon signed-rank) if distributional assumptions are a concern.

---

## Best Practices & Tips

- Keep file names consistent across model folders so that per-file comparisons line up naturally.
- Ensure every file contains the correct delimiter (`Output:` or `output:`), otherwise the output will be empty for that file.
- Inspect `qe_scores_wide.csv` for missing values (`NaN`) which indicate files that failed to score or parse.
- For very large batches, consider batching `predict` calls and using GPU if available.

---

## Troubleshooting

- **No model directories found:** Verify `BASE_DIR` and that each subfolder contains `.txt` files.
- **Parsing produced empty hypotheses:** Check delimiters in your `.txt`. Add variants to `PARSE_KEYS` if needed.
- **COMET errors / missing model:** The notebook tries `unbabel-comet` auto-install; re-run the install cell or install manually:
  ```bash
  pip install unbabel-comet
  ```
- **Statistical tests show NaN:** Too few matched pairs after dropping rows with missing scores; ensure consistent files across models.

---

## Acknowledgements

- **COMET QE**: This pipeline relies on the COMET QE checkpoint `wmt21-comet-qe-mqm`.

If you build upon this pipeline in academic or industrial work, please include an appropriate acknowledgement to COMET QE and this evaluation workflow.


